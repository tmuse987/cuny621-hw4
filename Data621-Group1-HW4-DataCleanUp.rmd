---
title: 'Data 621 Homework 3: Insurance'
author: "Tommy Jenkins, Violeta Stoyanova, Todd Weigel, Peter Kowalchuk, Eleanor R-Secoquian, Anthony Pagan"
date: "November 6, 2019"
output:
  pdf_document: default
  html_document:
    number_sections: yes
    theme: paper
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Get the data. Added na.strings to add na for records that have a blank value

InsTrain <- read.csv("insurance_training_data.csv",na.strings="",header=TRUE)
InsEval <- read.csv("insurance-evaluation-data.csv",na.strings="",header=TRUE)
InsEval <- subset(InsEval, select=-c(TARGET_FLAG,TARGET_AMT))
```

# OVERVIEW

In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

## Objective: 

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car.

# DATA EXPLORATION

## Data Summary 
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
ins1 <- describe(InsTrain, na.rm = F)
ins1$na_count <- sapply(InsTrain, function(y) sum(length(which(is.na(y)))))
ins1$na_count_perc <- sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
colsTrain<-ncol(InsTrain)
colsEval<-ncol(InsEval)
missingCol<-colnames(InsTrain)[!(colnames(InsTrain) %in% colnames(InsEval))]
```

The dataset consists of two data files: training and evaluation. The training dataset contains `r colsTrain` columns, while the evaluation dataset contains `r colsEval`. The evaluation dataset is missing columns `r missingCol` which represend our responce variables, respectively whether the person was in a car crash and the cost of the car crash if the person was in an accident. We will start by exploring the training data set since it will be the one used to generate the models.

The columns in the data set are:    
![](dataTable.png)

## Missing Data

An important aspect of any dataset is to determine how much, if any, data is missing. We look at all the variables to see which if any have missing data. We look at the basic descriptive statistics as well as the missing data and their percentages. 

We start by looking at the dataset as awhole and determine how many complete rows, that is rows with data for all predictors, do we have.


```{r echo=FALSE, message=FALSE, warning=FALSE}
cc<-summary(complete.cases(InsTrain))
cInsTrain<-subset(InsTrain, complete.cases(InsTrain))
cc
```

With these results, if we remove all rows with incomplete rows, there will be a total of `r as.integer(cc[3])` rows out of `r nrow(InsTrain)` .If we eliminate all non-complete rows and keep only rows with data for all the predictors in the dataset, our new dataset will results in `r round(as.integer(cc[3])/nrow(InsTrain),2)*100`% of the total dataset. We create a subset of data with complete cases only to use later in our analysis.


```{r}
glimpse(cInsTrain)
```


But we can also look at what specific predictors are missing in our dataset. If we do this we can see how there is much more data available, as we find only 5 predictors with missing data. Data missing for these predictors also only accounts for less than 7% of the respective predictors total.


```{r echo=FALSE,message=FALSE,warning=FALSE}
#sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
vis_miss(InsTrain)
```

We look closer at the missing data and look at the intersection of predictors with missing data. We find that the bulk of the missing data is for predictors with no intersection with other missing predictor data.

```{r echo=FALSE,message=FALSE,warning=FALSE}
gg_miss_upset(InsTrain)
```

Having this detail in missing data might be of importance when looking at models. In the next Data Preparation section we will handle these missing cases and build a data set with data for all predictors in all rows.

## Data Exploration 

Using TARGET_FLAG as response variables we confirm  when TARGET_FLAG is 1  TARGET_AMOUNT >0 and when TARGET_FLAG is 0 when TARGET_AMOUNT  = 0

```{r echo=TRUE}

nrow(subset(InsTrain,TARGET_FLAG == 0))
nrow(subset(InsTrain,TARGET_AMT == 0))
nrow(subset(InsTrain,TARGET_FLAG > 0))
nrow(subset(InsTrain,TARGET_AMT > 0))
```

A glimpse of the data shows that the following columns should be integers and not Factors:

* INCOME
* HOME_VAL
* BLUEBOOK
* OLDCLAIM
* JOB

We display and view data with all cases and only complete cases

```{r echo=FALSE, message=FALSE, warning=FALSE}
glimpse(InsTrain)
```

We use Sapply function to review which columns have NA Values. It display columns and percent of values that are missing.  

```{r}
sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```

## Data Preperation

As revealed earlier there were a list of columns that we factors that should be integers. We start by converting the columns to numeric.

```{r}
c<-c('INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'JOB')

glimpse(InsTrain[,colnames(InsTrain) %in% c])

InsTrain[,colnames(InsTrain) %in% c]<-data.frame(sapply(InsTrain[,colnames(InsTrain) %in% c], function(x) as.numeric(x)))

glimpse(InsTrain[,colnames(InsTrain) %in% c])

```

Both boxplot and summary stats with the square root transform of Home_val and Income to confirm we can use median or mean values to replace NA values if we chose.

```{r}
r <- colnames(InsTrain)[ apply(InsTrain, 2, anyNA)]

par(mar=c(9.5,3.5,.5,.5))

boxplot(InsTrain$AGE,InsTrain$YOJ, InsTrain$CAR_AGE, sqrt(InsTrain$INCOME), sqrt(InsTrain$HOME_VAL), InsTrain$JOB,names = r,las = 2,col = c("orange","red", "blue", "yellow", "brown", "green"))

describe(subset(InsTrain, select =r))

```

We next replace all NA values with mean values for cases that are missing values and rerun sapply function to confirm there are no longer any missing values.

```{r}

InsTrain[,colnames(InsTrain) %in% r]<-data.frame(sapply(InsTrain[,colnames(InsTrain) %in% r],
      function(x) ifelse(is.na(x),
            mean(x, na.rm = TRUE),
            x)))

sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
vis_miss(InsTrain)
describe(subset(InsTrain, select =r))
#View(InsTrain)
```

## Build Model

```{r}
m1<-glm(TARGET_FLAG~.-INDEX-TARGET_FLAG-TARGET_AMT,data=InsTrain,family="binomial"(link="logit"))
summary(m1)

```

# Select Model

## Compare Model Statistics

## Conclusion

# APPENDIX
