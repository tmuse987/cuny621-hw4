---
title: 'Data 621 Homework 3: Insurance'
author: "Tommy Jenkins, Violeta Stoyanova, Todd Weigel, Peter Kowalchuk, Eleanor R-Secoquian, Anthony Pagan"
date: "November 6, 2019"
output:
  pdf_document: default
  html_document:
    number_sections: yes
    theme: paper
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Get the data. Added na.strings to add na for records that have a blank value

InsTrain <- read.csv("insurance_training_data.csv",na.strings="",header=TRUE)
InsEval <- read.csv("insurance-evaluation-data.csv",na.strings="",header=TRUE)
InsEval <- subset(InsEval, select=-c(TARGET_FLAG,TARGET_AMT))
```

# OVERVIEW

In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero representing the cost of the crash.

## Objective: 

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car.

# DATA EXPLORATION

## Data Summary 
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
ins1 <- describe(InsTrain, na.rm = F)
ins1$na_count <- sapply(InsTrain, function(y) sum(length(which(is.na(y)))))
ins1$na_count_perc <- sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
colsTrain<-ncol(InsTrain)
colsEval<-ncol(InsEval)
missingCol<-colnames(InsTrain)[!(colnames(InsTrain) %in% colnames(InsEval))]
```

The dataset consists of two data files: training and evaluation. The training dataset contains `r colsTrain` columns, while the evaluation dataset contains `r colsEval`. The evaluation dataset is missing columns `r missingCol` which represent our response variables, respectively whether the person was in a car crash and the cost of the car crash if the person was in an accident. We will start by exploring the training data set since it will be the one used to generate the models.

The columns in the data set are:    
![](dataTable.png)

## Missing Data

An important aspect of any dataset is to determine how much, if any, data is missing. We look at all the variables to see which if any have missing data. We look at the basic descriptive statistics as well as the missing data and percentages. 

We start by looking at the dataset as a whole and determine how many complete rows, that is rows with data for all predictors, do we have.


```{r echo=FALSE, message=FALSE, warning=FALSE}
cc<-summary(complete.cases(InsTrain))
cInsTrain<-subset(InsTrain, complete.cases(InsTrain))
cc
```

With these results, if we remove all rows with incomplete rows, there will be a total of `r as.integer(cc[3])` rows out of `r nrow(InsTrain)` .If we eliminate all non-complete rows and keep only rows with data for all the predictors in the dataset, our new dataset will results in `r round(as.integer(cc[3])/nrow(InsTrain),2)*100`% of the total dataset. We create a subset of data with complete cases only to use later in our analysis.


```{r}
glimpse(cInsTrain)
```


But we can also look at what specific predictors are missing in our dataset. If we do this we can see how there is much more data available, as we find only 5 predictors with missing data. Data missing for these predictors also only accounts for less than 7% of the respective predictors total.


```{r echo=FALSE,message=FALSE,warning=FALSE}
#sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
vis_miss(InsTrain)
```

We look closer at the missing data and look at the intersection of predictors with missing data. We find that the bulk of the missing data is for predictors with no intersection with other missing predictor data.

```{r echo=FALSE,message=FALSE,warning=FALSE}
gg_miss_upset(InsTrain)
```

Having this detail in missing data might be of importance when looking at models. In the next Data Preparation section we will handle these missing cases and build a data set with data for all predictors in all rows.

## Data Exploration 

Using TARGET_FLAG as response variables we confirm  when TARGET_FLAG is 1  TARGET_AMOUNT >0 and when TARGET_FLAG is 0 when TARGET_AMOUNT  = 0

```{r echo=TRUE}

nrow(subset(InsTrain,TARGET_FLAG == 0))
nrow(subset(InsTrain,TARGET_AMT == 0))
nrow(subset(InsTrain,TARGET_FLAG > 0))
nrow(subset(InsTrain,TARGET_AMT > 0))
```

A glimpse of the data shows that the following columns should be integers and not Factors:

* INCOME
* HOME_VAL
* BLUEBOOK
* OLDCLAIM
* JOB

We display and view data with all cases and only complete cases

```{r echo=FALSE, message=FALSE, warning=FALSE}
glimpse(InsTrain)
```

We use Sapply function to review which columns have NA Values. It display columns and percent of values that are missing.  

```{r}
sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```

## Data Preperation

As revealed earlier there were a list of columns that we factors that should be integers. We start by converting the columns to numeric.

```{r}
c<-c('INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'JOB')

glimpse(InsTrain[,colnames(InsTrain) %in% c])

InsTrain[,colnames(InsTrain) %in% c]<-data.frame(sapply(InsTrain[,colnames(InsTrain) %in% c], function(x) as.numeric(x)))

glimpse(InsTrain[,colnames(InsTrain) %in% c])

```

Both boxplot and summary stats with the square root transform of Home_val and Income to confirm we can use median or mean values to replace NA values if we chose.

```{r}
r <- colnames(InsTrain)[ apply(InsTrain, 2, anyNA)]

par(mar=c(9.5,3.5,.5,.5))

boxplot(InsTrain$AGE,InsTrain$YOJ, InsTrain$CAR_AGE, sqrt(InsTrain$INCOME), sqrt(InsTrain$HOME_VAL), InsTrain$JOB,names = r,las = 2,col = c("orange","red", "blue", "yellow", "brown", "green"))

describe(subset(InsTrain, select =r))

```

We next replace all NA values with mean values for cases that are missing values and rerun sapply function to confirm there are no longer any missing values.

```{r}

InsTrain[,colnames(InsTrain) %in% r]<-data.frame(sapply(InsTrain[,colnames(InsTrain) %in% r],
      function(x) ifelse(is.na(x),
            mean(x, na.rm = TRUE),
            x)))

sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
vis_miss(InsTrain)
describe(subset(InsTrain, select =r))
#View(InsTrain)
```

We have this way derived a dataset with no missing values. We can use this set of data for our modeling design. We chose to work with this data as opposed to the first "complete" set in which rows with missing data were eliminated.

# Build Model

Modeling design will be divided in two phases. First we will design a model to predict if the person is in a car crash, that is predict TARGET_FLAG. In a second phase, we will predict TARGET_AMT, the cost of the crash.

##TARGET_FLAG Modeling

This response variable being binary, o or 1, we will be looking at logistic regression models to find a good fit. We will start with a naive model with all the predictors included as a baseline. First approach will be to simply the model by reducing the predictors used. We will look at several model metrics such as AIC, BIC. We will also include confusion tables and ROC plot to better understand each model.

**Model 1: all predictors**

We start out with a straightforward logit logistical regression with all predictors included. As a note, we need to make sure we do not include the TARGET_AMT responce variable in our model as a predictor.

```{r}
m1<-glm(TARGET_FLAG~.-INDEX-TARGET_AMT,data=InsTrain,family="binomial"(link="logit"))
summary(m1)
```

From the model's summary itself we see that there are several predictors which are not statistically relevant, which suggestes a simpler model should be possible.We build a second model without these the non-significant predictors.

```{r}
m2<-glm(TARGET_FLAG~.-INDEX-TARGET_AMT-AGE-INCOME-JOB-BLUEBOOK-CAR_AGE,data=InsTrain,family="binomial"(link="logit"))
summary(m2)
```

### Select Model

### Compare Model Statistics

##TARGET_AMT Modeling

### Select Model

### Compare Model Statistics

## Conclusion

# APPENDIX
