---
title: 'Data 621 Homework 4: Car Insurance'
author: Tommy Jenkins, Violeta Stoyanova, Todd Weigel, Peter Kowalchuk, Eleanor R-Secoquian,
  Anthony Pagan
date: "November 15, 2019"
output:
  pdf_document: default
  word_document: default
  html_document:
    number_sections: yes
    theme: paper
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Get the data. Added na.strings to add na for records that have a blank value
InsTrain <- read.csv("insurance_training_data.csv",na.strings="",header=TRUE)
InsEval <- read.csv("insurance-evaluation-data.csv",na.strings="",header=TRUE)
InsEval <- subset(InsEval, select=-c(TARGET_FLAG,TARGET_AMT))
```

```{r}
InsEval <- read.csv("insurance-evaluation-data.csv",na.strings="",header=TRUE)
```


# OVERVIEW

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero representing the cost of the crash.

## Objective: 

Our objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car.

# DATA EXPLORATION

## Data Summary 
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
ins1 <- describe(InsTrain, na.rm = F)
ins1$na_count <- sapply(InsTrain, function(y) sum(length(which(is.na(y)))))
ins1$na_count_perc <- sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
colsTrain<-ncol(InsTrain)
colsEval<-ncol(InsEval)
missingCol<-colnames(InsTrain)[!(colnames(InsTrain) %in% colnames(InsEval))]
```

The dataset consists of two data files: training and evaluation. The training dataset contains `r colsTrain` columns, while the evaluation dataset contains `r colsEval`. The evaluation dataset is missing columns `r missingCol` which represent our response variables, respectively whether the person was in a car crash and the cost of the car crash if the person was in an accident. We will start by exploring the training data set since it will be the one used to generate the models.

The columns in the data set are:    
![](dataTable.png)

## Missing Data

An important aspect of any dataset is to determine how much, if any, data is missing. We look at all the variables to see which if any have missing data. We look at the basic descriptive statistics as well as the missing data and percentages. 

We start by looking at the dataset as a whole and determine how many complete rows, that is rows with data for all predictors, do we have.


```{r echo=FALSE, message=FALSE, warning=FALSE}
cc<-summary(complete.cases(InsTrain))
cInsTrain<-subset(InsTrain, complete.cases(InsTrain))
cc
```

With these results, if we remove all rows with incomplete rows, there will be a total of `r as.integer(cc[3])` rows out of `r nrow(InsTrain)` .If we eliminate all non-complete rows and keep only rows with data for all the predictors in the dataset, our new dataset will results in `r round(as.integer(cc[3])/nrow(InsTrain),2)*100`% of the total dataset. We create a subset of data with complete cases only to use later in our analysis.


```{r}
glimpse(cInsTrain)
```


But we can also look at what specific predictors are missing in our dataset. If we do this we can see how there is much more data available, as we find only 5 predictors with missing data. Data missing for these predictors also only accounts for less than 7% of the respective predictors total.


```{r echo=FALSE,message=FALSE,warning=FALSE}
#sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
vis_miss(InsTrain)
```

We look closer at the missing data and look at the intersection of predictors with missing data. We find that the bulk of the missing data is for predictors with no intersection with other missing predictor data.

```{r echo=FALSE,message=FALSE,warning=FALSE}
gg_miss_upset(InsTrain)
```

Having this detail in missing data might be of importance when looking at models. In the next Data Preparation section we will handle these missing cases and build a data set with data for all predictors in all rows.

## Data Exploration 

Using TARGET_FLAG as response variables we confirm  when TARGET_FLAG is 1  TARGET_AMOUNT >0 and when TARGET_FLAG is 0 when TARGET_AMOUNT  = 0

```{r echo=TRUE}
nrow(subset(InsTrain,TARGET_FLAG == 0))
nrow(subset(InsTrain,TARGET_AMT == 0))
nrow(subset(InsTrain,TARGET_FLAG > 0))
nrow(subset(InsTrain,TARGET_AMT > 0))
```

A glimpse of the data shows that the following columns should be integers and not Factors:

* INCOME
* HOME_VAL
* BLUEBOOK
* OLDCLAIM

We display and view data with all cases and only complete cases

```{r echo=FALSE, message=FALSE, warning=FALSE}
cat(colnames(InsTrain[ sapply(InsTrain, is.factor)]), "\n\n")
glimpse(InsTrain)
```

We use sapply function to review which columns have NA Values. It display columns and percent of values that are missing.  

```{r}
sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
```

## Data Preparation

As revealed earlier there were a list of columns that we factors that should be integers. We start by converting the columns to numeric.

```{r}
c<-c('INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM')
if(c %in% colnames(InsTrain)){
  
  glimpse(InsTrain[,(c)])
  InsTrain[,c] <- sapply(InsTrain[,(c)], 
                         function(x) as.integer(gsub('[$,]','',as.character(x))))
  
  glimpse(InsTrain[,(c)])
  
} else {
  
  cat("Please review your selection of columns:", c)
  
}
```

Both boxplot and summary stats with the square root transform of Home_val and Income to confirm we can use median or mean values to replace NA values if we chose.

```{r}
# AGE YOJ INCOME HOME_VAL CAR_AGE
InsTrain$INCOME <- na_if(InsTrain$INCOME, 0)
InsTrain$HOME_VAL <- na_if(InsTrain$HOME_VAL, 0)
r <- colnames(InsTrain[ sapply(InsTrain, function(x) return(anyNA(x) && is.integer(x)))])
boxplot(InsTrain[,r],names = r,las = 2,col = c("orange","red", "blue", "yellow", "brown", "green"))
describe(subset(InsTrain, select =r))
```

We next replace all NA values with mean values for cases that are missing values, we impute the JOBs variable for now (although we will reconsider whether omitting these rows is better) and rerun sapply function to confirm there are no longer any missing values.


```{r}
sapply(InsTrain, function(x) round(sum(is.na(x))/nrow(InsTrain)*100,1))
InsTrain[,r] <- replace_na(InsTrain[,r], as.list(colMeans(InsTrain[,r], na.rm = TRUE)))

# Jobs could be analyzed more before imputing
Jobs <- summary(InsTrain$JOB)
JobsMode <- Jobs[which.max(Jobs)] 
# ifelse(JobsMode[[1]]  / nrow(InsTrain) > 2.5*(Jobs["NA's"][[1]] / nrow(InsTrain)), 
InsTrain$JOB <- replace_na(InsTrain$JOB, names(JobsMode))#,
#na.omit(InsTrain)
#summary(InsTrain$JOB)
```


```{r}
vis_miss(InsTrain)
describe(subset(InsTrain, select =r))
#View(InsTrain)
```

We have this way derived a dataset with no missing values. We can use this set of data for our modeling design. We chose to work with this data as opposed to the first "complete" set in which rows with missing data were eliminated.

# Build Model

Modeling design will be divided in two phases. First we will design a model to predict if the person is in a car crash, that is predict TARGET_FLAG. In a second phase, we will predict TARGET_AMT, the cost of the crash.

## TARGET_FLAG Modeling

This response variable being binary, 0 or 1, we will be looking at logistic regression models to find a good fit. We will start with a naive model with all the predictors included as a baseline. First approach will be to simply the model by reducing the predictors used. We will look at several model metrics such as AIC, BIC. We will also include confusion tables and ROC plot to better understand each model.

**Model 1: all predictors**

We start out with a straightforward logit logistical regression with all predictors included. As a note, we need to make sure we do not include the TARGET_AMT response variable in our model as a predictor.

```{r}
m1<-glm(TARGET_FLAG~.-INDEX-TARGET_AMT,data=InsTrain,family="binomial"(link="logit"))
summary(m1)
```

From the model's summary itself we see that there are several predictors which are not statistically relevant, which suggest a simpler model should be possible. We build a second model without the non-significant predictors.

**Model 2: reduced predictors**

```{r}
m2<-glm(TARGET_FLAG~.-INDEX-TARGET_AMT-AGE-INCOME-JOB-BLUEBOOK-CAR_AGE-RED_CAR,data=InsTrain,family="binomial"(link="logit"))
summary(m2)
```

The new model has a slightly higher AIC which would tells us the first model is slightly less complex.

### AIC Step Method Model 3

Another way of selecting which predictors to use in the model is by calculating the AIC of the model. This metric is similar to the adjusted R-square of a model in that it penalizes models with more predictors over simpler model with few predictors. We use Stepwise function in r to find the lowest AIC with different predictors.

```{r echo=FALSE,message=FALSE,warning=FALSE}
m3 <- step(m1)
summary(m3)
```

This reduces the predictors used to 25 from 30. The AIC is reduced from 7401.13 (our original general model) to 7393.7, just slightly and but we benefit by having a simpler model less prone to overfitting.

Also, the predictors in the model now are all significant (under 0.05 pr level) and all but one under .02 or very significant. Which is much improved over the first model.

### BIC Method  Model 4

To determine the number of predictors and which predictors to be used we will use the Bayesian Information Criterion (BIC).

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsTrainM4<-InsTrain[ , !(names(InsTrain) %in% c('INDEX','TARGET_AMT'))]
regfit.full <- regsubsets(factor(TARGET_FLAG) ~ ., data=InsTrainM4)
par(mfrow = c(1,2))
reg.summary <- summary(regfit.full)
plot(reg.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l", main="Subset Selection Using BIC")
BIC_num <- which.min(reg.summary$bic) 
points(BIC_num, reg.summary$bic[BIC_num], col="red", cex=2, pch=20)
```

```{r echo=FALSE,message=FALSE,warning=FALSE,fig.height=7,fig.width=8}
plot(regfit.full, scale="bic", main="Predictors vs. BIC", asp = 10)
```

The plot on the right shows that the number of predictors with the lowest BIC are `PARENT` , `HOMEVAL`, `CAR_USE`, 'CAR_TYPE', 'REVOKED', 'MVR_PTS', 'CAR_AGE' and 'URBANICITY'. We will use those predictors to build the next model

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsTrains <-subset(InsTrain, PARENT1='YES',CAR_TYPE %in% c('Minivan','Pickup','Sports Car','Van','z_SUV'), REVOKED = 'YES', CAR_USE = 'Private')
m4 <- glm(TARGET_FLAG ~ PARENT1 + HOME_VAL + CAR_USE + CAR_TYPE + REVOKED + MVR_PTS + URBANICITY + CAR_AGE, family=binomial, data = InsTrains)
InsTrains$predicted_m3<- predict(m4, InsTrains, type='response')
InsTrains$target_m4$target <- ifelse(InsTrains$predicted_m4>0.5, 1, 0)
pander::pander(summary(m4))
```

### Select Model
### Compare Model Statistics

### Model 1 - General Model

**ROC Curve**

The ROC Curve helps measure true positives and true negative. A high AUC or area under the curve tells us the model is predicting well. 

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat<-predict(m1,type="response")
g<-roc(TARGET_FLAG~targethat,data=InsTrain)
plot(g)
```
 
The AUC value of `r round(g$auc,2)`, tells us this model predicted values are accurate.

**Confusion Matrix**

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat[targethat<0.5]<-0
targethat[targethat>=0.5]<-1
table(targethat,InsTrain$TARGET_FLAG)
```

**Create a binned diagnostic plot of residuals vs prediction**
There are definite patterns here, which bear investigating.

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsMut <- mutate(InsTrain, Residuals = residuals(m1), linPred = predict(m1))
grpIns <- group_by(InsMut, cut(linPred, breaks=unique(quantile(linPred, (0:25/26)))))
diagIns <- summarise(grpIns, Residuals = mean(Residuals), linPred = mean(linPred))
plot(Residuals ~ linPred, data = diagIns, xlab="Linear Predictor")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
abline(lm(Residuals ~ linPred, data = diagIns))
```

**Plot leverages.**

```{r echo=FALSE,message=FALSE,warning=FALSE}
halfnorm(hatvalues(m1))
```

We don't see any strong outliers with the leverage plot.  The points identified (3608,5686) are essentially in the plot of the line formed, so they are not likely pulling our model in any direction.

**Plot Goodness of fit**

```{r echo=FALSE,message=FALSE,warning=FALSE}
linPred <- predict(m1)
InsMut <- mutate(InsTrain, predProb = predict(m1, type = "response"))
grpIns <- group_by(InsMut, cut(linPred, breaks = unique(quantile(linPred, (0:25)/26))))
#hosmer-lemeshow stat
hlDf <- summarise(grpIns, y= sum(TARGET_FLAG), pPred=mean(predProb), count = n())
hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))
ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")
```

We see that our predictors fall close to the line. 

### Model 2 - Reduced General Model

**ROC Curve**

The ROC Curve helps measure true positives and true negative. A high AUC or area under the curve tells us the model is predicting well. 

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat<-predict(m2,type="response")
g<-roc(TARGET_FLAG~targethat,data=InsTrain)
plot(g)
```
 
The AUC value of `r round(g$auc,2)`, tells us this model predicted values are curate.

**Confusion Matrix**

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat[targethat<0.5]<-0
targethat[targethat>=0.5]<-1
table(targethat,InsTrain$TARGET_FLAG)
```

**Create a binned diagnostic plot of residuals vs prediction**
There are definite patterns here, which bear investigating.

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsMut <- mutate(InsTrain, Residuals = residuals(m2), linPred = predict(m2))
grpIns <- group_by(InsMut, cut(linPred, breaks=unique(quantile(linPred, (0:25/26)))))
diagIns <- summarise(grpIns, Residuals = mean(Residuals), linPred = mean(linPred))
plot(Residuals ~ linPred, data = diagIns, xlab="Linear Predictor")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
abline(lm(Residuals ~ linPred, data = diagIns))
```

**Plot leverages.**

```{r echo=FALSE,message=FALSE,warning=FALSE}
halfnorm(hatvalues(m2))
```

We don't see any strong outliers with the leverage plot.  The points identified (3608,5686) are essentially in the plot of the line formed, so they are not likely pulling our model in any direction.

**Plot Goodness of fit**

```{r echo=FALSE,message=FALSE,warning=FALSE}
linPred <- predict(m2)
InsMut <- mutate(InsTrain, predProb = predict(m2, type = "response"))
grpIns <- group_by(InsMut, cut(linPred, breaks = unique(quantile(linPred, (0:25)/26))))
#hosmer-lemeshow stat
hlDf <- summarise(grpIns, y= sum(TARGET_FLAG), pPred=mean(predProb), count = n())
hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))
ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")
```

We see that our predictors fall close to the line. 

### Model 3 - Step AIC Model

**ROC Curve**

The ROC Curve helps measure true positives and true negative. A high AUC or area under the curve tells us the model is predicting well. 

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat<-predict(m3,type="response")
g<-roc(TARGET_FLAG~targethat,data=InsTrain)
plot(g)
```
 
The AUC value of `r round(g$auc,2)`, tells us this model predicted values are accurate.

**Confusion Matrix**

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat[targethat<0.5]<-0
targethat[targethat>=0.5]<-1
table(targethat,InsTrain$TARGET_FLAG)
```

**Create a binned diagnostic plot of residuals vs prediction**
There are definite patterns here, which bear investigating.

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsMut <- mutate(InsTrain, Residuals = residuals(m3), linPred = predict(m3))
grpIns <- group_by(InsMut, cut(linPred, breaks=unique(quantile(linPred, (0:25/26)))))
diagIns <- summarise(grpIns, Residuals = mean(Residuals), linPred = mean(linPred))
plot(Residuals ~ linPred, data = diagIns, xlab="Linear Predictor")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
abline(lm(Residuals ~ linPred, data = diagIns))
```

**Plot leverages.**

```{r echo=FALSE,message=FALSE,warning=FALSE}
halfnorm(hatvalues(m3))
```

We don't see any strong outliers with the leverage plot.  The points identified (3608,5686) are essentially in the plot of the line formed, so they are not likely pulling our model in any direction.

**Plot Goodness of fit**

```{r echo=FALSE,message=FALSE,warning=FALSE}
linPred <- predict(m3)
InsMut <- mutate(InsTrain, predProb = predict(m3, type = "response"))
grpIns <- group_by(InsMut, cut(linPred, breaks = unique(quantile(linPred, (0:25)/26))))
#hosmer-lemeshow stat
hlDf <- summarise(grpIns, y= sum(TARGET_FLAG), pPred=mean(predProb), count = n())
hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))
ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")
```

We see that our predictors fall close to the line. 

### Model 4 - Rep BIC Model

**ROC Curve**

The ROC Curve helps measure true positives and true negative. A high AUC or area under the curve tells us the model is predicting well. 

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat<-predict(m4,type="response")
g<-roc(TARGET_FLAG~targethat,data=InsTrains)
plot(g)
```
 
The AUC value of `r round(g$auc,2)`, tells us this model predicted values are accurate.

**Confusion Matrix**

```{r echo=FALSE,message=FALSE,warning=FALSE}
targethat[targethat<0.5]<-0
targethat[targethat>=0.5]<-1
table(targethat,InsTrains$TARGET_FLAG)
```

**Create a binned diagnostic plot of residuals vs prediction**
There are definite patterns here, which bear investigating.

```{r echo=FALSE,message=FALSE,warning=FALSE}
InsMut <- mutate(InsTrains, Residuals = residuals(m4), linPred = predict(m4))
grpIns <- group_by(InsMut, cut(linPred, breaks=unique(quantile(linPred, (0:25/26)))))
diagIns <- summarise(grpIns, Residuals = mean(Residuals), linPred = mean(linPred))
plot(Residuals ~ linPred, data = diagIns, xlab="Linear Predictor")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
abline(lm(Residuals ~ linPred, data = diagIns))
```

**Plot leverages.**

```{r echo=FALSE,message=FALSE,warning=FALSE}
halfnorm(hatvalues(m4))
```

We don't see any strong outliers with the leverage plot.  The points identified (3608,5686) are essentially in the plot of the line formed, so they are not likely pulling our model in any direction.

**Plot Goodness of fit**

```{r echo=FALSE,message=FALSE,warning=FALSE}
linPred <- predict(m4)
InsMut <- mutate(InsTrains, predProb = predict(m4, type = "response"))
grpIns <- group_by(InsMut, cut(linPred, breaks = unique(quantile(linPred, (0:25)/26))))
#hosmer-lemeshow stat
hlDf <- summarise(grpIns, y= sum(TARGET_FLAG), pPred=mean(predProb), count = n())
hlDf <- mutate(hlDf, se.fit=sqrt(pPred * (1-(pPred)/count)))
ggplot(hlDf,aes(x=pPred,y=y/count,ymin=y/count-2*se.fit,ymax=y/count+2*se.fit)) +
    geom_point()+geom_linerange(color=grey(0.75))+geom_abline(intercept=0,slope=1) +
    xlab("Predicted Probability") +
    ylab("Observed Proportion")
```

We see that our predictors fall close to the line.

### Pick the best regression model

```{r echo=FALSE,message=FALSE,warning=FALSE}
m1AIC <- AIC(m1)
m1BIC <- BIC(m1)
m2AIC <- AIC(m2)
m2BIC <- BIC(m2)
m3AIC <- AIC(m3)
m3BIC <- BIC(m3)
m4AIC <- AIC(m4)
m4BIC <- BIC(m4)
```
| Metric    | Model 1     | Model 2      | Model 3    | Model 4    |
| --------- | ----------- | -----------  | ---------- | ---------- |
| AIC       | `r m1AIC`   | `r m2AIC`    | `r m3AIC`  | `r m4AIC`  |
| BIC       | `r m1BIC`   | `r m2BIC`    | `r m3BIC`  | `r m4BIC`  |

With 4 models computed, we select the model with the lowest combination of AIC and BIC. The the table we can see the model to pick is model 3.

## TARGET_AMT Modeling

**Model 1: all predictors**

Same as with the logistic model before, we start with a model that includes all predictors

```{r}
InsTrain<-InsTrain[ , !(names(InsTrain) %in% c('predicted_m3','target_m4'))]
lm1<-lm(TARGET_AMT~.-TARGET_FLAG,InsTrain)
summary(lm1)
par(mfrow = c(2,2))
plot(lm1)
```

This model shows an adj $R^2$ as 0.28, and F-statistic of 87.86 with a small p-value. The result is not a very good model showing a very low $R^2$. We also observe several parameters which are not very significant. We try a second model without these parameters, although we do not expect it so be much better that this first model.

**Model 2: Significant predictors**

```{r}
lm2 <- lm(TARGET_AMT ~ +AGE +EDUCATION +REVOKED +MVR_PTS +JOB +YOJ +CLM_FREQ +HOME_VAL +URBANICITY +PARENT1 +MSTATUS +TRAVTIME +BLUEBOOK, data = InsTrain)
summary(lm2)
par(mfrow = c(2,2))
plot(lm2)
```

This model shows an adj $R^2$ as 0.056, and F-statistic of 22.86 with a small p-value.

Using the reduced predictors, let's now do a stepwise regression:

**Model 3: Stepwise Regression **
```{r echo=FALSE,message=FALSE,warning=FALSE}
lm3 <- step(lm2)
summary(lm3)
par(mfrow = c(2,2))
plot(lm3)
```

This model shows an adj $R^2$ as 0.055, and F-statistic of 32.86 with a small p-value. As expected this model isn't any better than the first one. It is simpler, but its performance is basically the same. What we do notice, and very visible in the Q-Q plot, is that these seem to be a high number of data points distance from the normal. This suggest a different kind of model.


**Model 4: Weighted coefficients**

We build a Hubber weighted model to account for the distant points observed in the previous models.

```{r}
lm4<-rlm(TARGET_AMT~.-TARGET_FLAG,InsTrain,maxit=40)
summary(lm4)
par(mfrow = c(2,2))
plot(lm4)
```

The models doesn't seem to help with weighted coefficients, we still see the effects of several datapoints in the data.

### Pick the best regression model

```{r echo=FALSE,message=FALSE,warning=FALSE}
lm1AIC <- AIC(lm1)
lm1BIC <- BIC(lm1)
lm2AIC <- AIC(lm2)
lm2BIC <- BIC(lm2)
lm3AIC <- AIC(lm3)
lm3BIC <- BIC(lm3)
lm4AIC <- AIC(lm4)
lm4BIC <- BIC(lm4)
```
| Metric    | Model 1     | Model 2      | Model 3    | Model 4    |
| --------- | ----------- | -----------  | ---------- | ---------- |
| AIC       | `r lm1AIC`   | `r lm2AIC`    | `r lm3AIC`  | `r lm4AIC`  |
| BIC       | `r lm1BIC`   | `r lm2BIC`    | `r lm3BIC`  | `r lm4BIC`  |

With 4 models computed, we select the model with the lowest combination of AIC and BIC. The model to pick is model 3.

## Conclusion

For both the logistic regression and linear regressions we picked model 3. Results for the logistic regression were rather good, but the linear regression doesn't seem to be a good model, even when using weighted coefficients. 

```{r message=FALSE, include=FALSE}
# TJ
c<-c('INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM')
if(c %in% colnames(InsEval)){
  
  glimpse(InsEval[,(c)])
  InsEval[,c] <- sapply(InsEval[,(c)], 
                         function(x) as.integer(gsub('[$,]','',as.character(x))))
  
  glimpse(InsEval[,(c)])
  
} else {
  
  cat("Please review your selection of columns:", c)
  
}
eval_plm<-predict(lm3,InsEval)
write.csv(eval_plm,"predicted_eval_values_Target_Amt.csv")
```

```{r echo = FALSE, message=FALSE}
eval_p<-predict(m3,InsEval, type = "response")
write.csv(eval_p,"predicted_eval_values_Target_Flag.csv")
```


# APPENDIX

**Code used in analysis**
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```

